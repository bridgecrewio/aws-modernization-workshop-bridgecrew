<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scenario introduction on Cloud DevSecOps workshop with Bridgecrew</title>
    <link>/kubernetes/30_scenario_walkthrough.html</link>
    <description>Recent content in Scenario introduction on Cloud DevSecOps workshop with Bridgecrew</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language><atom:link href="/kubernetes/30_scenario_walkthrough/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ArgoCD</title>
      <link>/kubernetes/30_scenario_walkthrough/3001_argo_cd.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/kubernetes/30_scenario_walkthrough/3001_argo_cd.html</guid>
      <description>In order to access the ArgoCD web interface within our Kubernetes cluster, we&amp;rsquo;ll need to know the public IP address of our workshop environment.
Clicking the link below, will load the CloudFormation stacks page. Select bridgecrew-workshop and select the Outputs tab, where we will see the public IP and ArgoCD URL.
https://us-east-2.console.aws.amazon.com/cloudformation/home?region=us-east-2#/stacks?filteringStatus=active&amp;amp;filteringText=&amp;amp;viewNested=true&amp;amp;hideStacks=false
The URL will be in the form https://&amp;lt;CLUSTER IP&amp;gt;:32443 click this link and you will be prompted with the ArgoCD login screen.</description>
    </item>
    
    <item>
      <title>Bridgecrew Code Reviews</title>
      <link>/kubernetes/30_scenario_walkthrough/3002_bridgecrew_code_review.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/kubernetes/30_scenario_walkthrough/3002_bridgecrew_code_review.html</guid>
      <description>Each time ArgoCD tries to deploy our development environment to the Kubernetes cluster, the cluster is instructed to check the security posture of incoming Kubernetes manifests with the Bridgecrew admission controller.
Details of all the issues found with the development deployment can be seen in this view, with filters for severity, tags, and other important attributes.
We’ve detected 17 issues on our Deployment object and no issues on the service object.</description>
    </item>
    
    <item>
      <title>Automation deep dive</title>
      <link>/kubernetes/30_scenario_walkthrough/3003_automation_deepdive.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/kubernetes/30_scenario_walkthrough/3003_automation_deepdive.html</guid>
      <description>While doing things manually is NOT what we want for a DevSecOps pipeline, for the sake of understanding the current setup a little more, lets see what it looks like if the developer was to try and apply the dev environment directly to the kubernetes cluster using the CLI, kubectl.
This will render the kustomize template and pass it to the kubernetes API, we should see the same results from the API as we saw through Argo, with our admission controller rejecting the deployment…</description>
    </item>
    
    <item>
      <title>Scenario summary</title>
      <link>/kubernetes/30_scenario_walkthrough/3004_scenario_summary.html</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/kubernetes/30_scenario_walkthrough/3004_scenario_summary.html</guid>
      <description>Where are we now? To summarize, we now have a free admission controller installed into our Kubernetes cluster to prevent issues from being deployed. Let’s take a look at where all this is happening in the development cycle:
So far, we’ve blocked the automated deployment (n the red oval). However, before we could block deployment, our team already spent a lot of time developing and testing the code we blocked.</description>
    </item>
    
  </channel>
</rss>
